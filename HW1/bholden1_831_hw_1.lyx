#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
customHeadersFooters
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\align center
Ben Holden
\end_layout

\begin_layout Standard
\align center
Robotics 831, Homework #1
\end_layout

\begin_layout Standard
\align center
9/22/2015
\end_layout

\begin_layout Standard
1) Introduction
\end_layout

\begin_layout Standard
2) Theory Questions
\end_layout

\begin_layout Standard
2.1) Realizability
\end_layout

\begin_layout Standard
Realizability is the assumption that (1) there exists a perfect classifier
 inside the hypothesis class, meaning there is a classifier which will always
 map the observations to the correct outcome and (2) every outcome is generated
 by a target mapping from the observations.
\end_layout

\begin_layout Standard
2.2) Hypothesis Class
\end_layout

\begin_layout Standard
A Hypthesis Class is set of target mapping functions, meaning it is the
 set of all of the functions, which map observations to possible outcomes.
 It is finite.
 This is important because it allows algorithms to weight the different
 hypothesises in order to actually have the learner learn instead of having
 a completely uninformed guess everytime.
\end_layout

\begin_layout Standard
2.3) Consistent (Greedy) Algorithm Regret Bound
\end_layout

\begin_layout Standard
The upper bound on the number of mistakes made by always picking the best
 expert is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M_{1}=N\cdot m^{*}
\]

\end_inset


\end_layout

\begin_layout Standard
This equation comes from the fact that when the learner has made M mistakes
 the best expert has made at most M/N mistakes or in equation form:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
m^{*}\le\frac{M_{1}}{N}
\]

\end_inset


\end_layout

\begin_layout Standard
So the worst case is when you pick the best expert it is the time that he
 makes a mistake giving:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M_{1}=N\cdot m^{*}
\]

\end_inset


\end_layout

\begin_layout Standard
The only time that you don't pick the best expert is when there is a tiebreaker.
 For this equation assume that everytime a tiebreaker occurs the worst case
 occurs and the learner picks the wrong expert to listen to and that expert
 is wrong.
 This gives the equation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M_{2}\le N-1
\]

\end_inset


\end_layout

\begin_layout Standard
Because if all experts are tied in mistakes and the best expert is the last
 index then N-1 mistakes will be made before reaching the last expert.
 The total upper bound on the number of mistakes is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M=M_{1}+M_{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Or:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\boxed{M\le N\cdot m^{*}+(N-1)}
\]

\end_inset


\end_layout

\begin_layout Standard
2.4) Understanding the 
\begin_inset Formula $\eta$
\end_inset

 (penalty) parameter
\end_layout

\begin_layout Standard
(1) Optimal 
\begin_inset Formula $\eta$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{d}{d\eta}\left(E(R)\right)=0=\frac{d}{d\eta}\left(\eta m^{*}+\frac{\ln N}{\eta}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
0=m^{*}-\frac{\ln N}{\eta^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\boxed{\eta=\left(\frac{\ln N}{m^{*}}\right)^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
(2) T << N; Should 
\begin_inset Formula $\eta$
\end_inset

 be big or small
\end_layout

\begin_layout Standard
If T is far smaller than N then 
\begin_inset Formula $\eta$
\end_inset

 should be picked to be large because the numerator in the answer above
 will dominate and if 
\begin_inset Formula $\eta$
\end_inset

 is big then it will effectively eliminate bad experts really fast, which
 is useful when there are a lot of experts.
\end_layout

\begin_layout Standard
(3) Picking 
\begin_inset Formula $\eta$
\end_inset

 with different orders of 
\begin_inset Formula $m^{*}$
\end_inset


\end_layout

\begin_layout Standard
Case: 
\begin_inset Formula $m^{*}=O(T)$
\end_inset

:
\end_layout

\begin_layout Standard
Pick 
\begin_inset Formula $\eta$
\end_inset

 to be small because the best expert and all experts in general are making
 a lot of mistakes (relative to the number of prediction rounds), so can't
 punish too hard.
\end_layout

\begin_layout Standard
Case: 
\begin_inset Formula $m^{*}=O(\sqrt{T})$
\end_inset

 :
\end_layout

\begin_layout Standard
Pick
\begin_inset Formula $\eta$
\end_inset

 to be large because the best expert is making a small number of mistakes
 (relative to the number of prediction rounds), so can punish mistakes more
 aggressively.
\end_layout

\begin_layout Standard
2.5) Understanding Adversarial Environments
\end_layout

\begin_layout Standard
(1) The adversary will maximize loss when it picks the minority label because
 the learner uses weighted majority to pick the label and that will be the
 opposite choice as the minority label.
\end_layout

\begin_layout Standard
(2) The largest expected loss is T or the number of prediction rounds.
 This is because the learner cannot assume realizability and there could
 be the case where every expert is wrong for every prediction round.
 This would mean that no matter the weights the learner would always pick
 the wrong answer because the weight for the correct answer is 0 meaning
 a 0% chance to sample that prediction from the distribution and a 100%
 chance to sample the wrong answer from the distribution.
 Randomization helps because in any other sitution (where at least 1 expert
 chooses differently from the rest) there will be a probability greater
 than 0% that the learner will pick the same as the adversary, whereas in
 weighted majoirty the learner would have always picked the greatest distibution.
\end_layout

\begin_layout Standard
3) Coding
\end_layout

\begin_layout Standard
3.1) Instructions
\end_layout

\begin_layout Standard
3.2) Programming Nature
\end_layout

\begin_layout Standard
3.3) WMA
\end_layout

\begin_layout Standard
Stochastic, WMA
\end_layout

\begin_layout Standard
The loss graphs show that about half of the predictions are incorrect.
 This is due to nature's stochastic behavior of 50% chance of a win.
 It makes sense that the regret is small because each expert is wrong about
 50% of the time (nature is stochastic) and the learner is wrong about 50%
 of the time.
 So the difference should be small.
 For a high eta and a high number of games these trends look the same, because
 the experts are not very smart (so eta doesn't do much) and nature is stochasti
c so the patterns continue with large numbers of games.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename C:/Users/Ben/Desktop/WMA_stochastic.png
	width 100col%

\end_inset


\end_layout

\begin_layout Standard
Deterministic, WMA
\end_layout

\begin_layout Standard
The loss graphs show that about a third of the predictions are incorrect.
 This is due to nature's deterministic behavior of every third game is a
 win.
 It makes sense that the regret is small because each expert is wrong about
 1/3 of the time (nature is deterministic) and the learner is wrong about
 1/3 of the time.
 So the difference should be small.
 For a high eta and a high number of games these trends look the same, because
 the experts are not very smart (so eta doesn't do much) and nature is determini
stic and repeats every third game so the patterns continue with large numbers
 of games.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename C:/Users/Ben/Desktop/WMA_deterministic.png
	width 100col%

\end_inset


\end_layout

\begin_layout Standard
Adversarial, WMA
\end_layout

\begin_layout Standard
The loss graphs show that everytime the learner is incorrect.
 This is due to nature's adversarial behavior of picking the minority out
 of the experts.
 It makes sense that the regret is half the number of predictions because
 each expert is wrong half of the time (nature is adversarial) and the learner
 is wrong all of the time.
 So the difference should be half.
 For a high eta and a high number of games these trends look the same, because
 the experts are not very smart (so eta doesn't do much) and nature is adversari
al and this nature dictates how WMA will do for any eta or game size.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename C:/Users/Ben/Desktop/WMA_adversarial.png
	width 100col%

\end_inset


\end_layout

\begin_layout Standard
3.4) RWMA
\end_layout

\begin_layout Standard
Stochastic, RWMA
\end_layout

\begin_layout Standard
The loss graphs show that about half of the predictions are incorrect.
 This is due to nature's stochastic behavior of 50% chance of a win.
 It makes sense that the regret is small because each expert is wrong about
 50% of the time (nature is stochastic) and the learner is wrong about 50%
 of the time.
 So the difference should be small.
 However, it is larger than the WMA algorithm due to the randomness of the
 algorithm.
 For a high eta and a high number of games these trends look the same, because
 the experts are not very smart (so eta doesn't do much) and nature is stochasti
c so the patterns continue with large numbers of games.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename C:/Users/Ben/Desktop/RWMA_stochastic.png
	width 100col%

\end_inset


\end_layout

\begin_layout Standard
Deterministic, RWMA
\end_layout

\begin_layout Standard
The loss graphs show that about a third of the predictions are incorrect.
 This is due to nature's deterministic behavior of every third game is a
 win.
 It makes sense that the regret is zero because each expert is wrong about
 1/3 of the time (nature is deterministic) and the learner is wrong 1/3
 of the time.
 So the difference should be 0.
 For a high eta and a high number of games these trends look the same, because
 the experts are not very smart (so eta doesn't do much) and nature is determini
stic and repeats every third game so the patterns continue with large numbers
 of games.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename C:/Users/Ben/Desktop/RWMA_deterministic.png
	width 100col%

\end_inset


\end_layout

\begin_layout Standard
Adversarial, RWMA
\end_layout

\begin_layout Standard
The loss graphs show that half of the time the learner is incorrect.
 This is due to nature's adversarial behavior of picking the minority out
 of the experts and then the random part of the algorithm being right about
 half of the time.
 It makes sense that the regret is 0 because the experts are wrong the same
 amount due to the adversarial part of this.
 For a high eta and a high number of games these trends look the same, because
 the experts are not very smart (so eta doesn't do much) and nature is adversari
al.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename C:/Users/Ben/Desktop/RWMA_adversarial.png
	width 100col%

\end_inset


\end_layout

\begin_layout Standard
3.5) Added the observation of the team's win percentage.
 Added the observation of whether the team is home or away.
 Added an expert who says if home then win.
 Added an expert who says if win percentage is greater than 50% then win.
 Added an expert who combines home and win percentage to say if win.
 Nature is deterministic and says win if a combination of weights from observati
ons are greater than a threshold.
 
\end_layout

\begin_layout Standard
Deterministic, WMA
\end_layout

\begin_layout Standard
Now the WMA does really well because of how the weights for the deterministic
 part of nature play out when compared to the experts.
 This gives Loss and Regret with finite bounds.
 The loss is about 20% and the regret is practically 0, because the experts
 are very good.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename C:/Users/Ben/Desktop/3_5_WMA.png
	width 100col%

\end_inset


\end_layout

\begin_layout Standard
Deterministic, RWMA
\end_layout

\begin_layout Standard
Now the RWMA does a little better because there are great experts, but the
 randomness actually makes the algorithm perform worse than the WMA.
 The loss and the regret are similar because there is are really good experts.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename C:/Users/Ben/Desktop/3_5_RWMA.png
	width 100col%

\end_inset


\end_layout

\end_body
\end_document
